{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "使用\\test_dir\\milvus_docs\\en\\faq下的四个markdown文档测试",
   "id": "806eab8f2f4082fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "先创建两个函数，将文本内容转化为向量，这个步骤和embeddings_test.ipynb中差不多",
   "id": "5bb954bd5eb821c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-25T13:01:55.429092Z",
     "start_time": "2024-12-25T13:01:52.315171Z"
    }
   },
   "source": [
    "from pymilvus import model\n",
    "\n",
    "sentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = 'all-MiniLM-L6-v2',\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "def encode_document(doc):\n",
    "    doc_embedding = sentence_transformer_ef.encode_documents([doc])\n",
    "    return doc_embedding[0]\n",
    "\n",
    "def encode_query(query):\n",
    "    query_embedding = sentence_transformer_ef.encode_queries(query)\n",
    "    print(query_embedding[0])\n",
    "    return query_embedding[0]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "读取md文件中的数据，用上面的两个函数生成嵌入向量，然后用milvus存储",
   "id": "8c2a5cb1023eb573"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:49:16.779451Z",
     "start_time": "2024-12-25T12:49:16.713422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymilvus import MilvusClient\n",
    "from pymilvus import connections,db\n",
    "conn = connections.connect(host=\"127.0.0.1\", port=19530)\n",
    "database = db.create_database(\"learn\")"
   ],
   "id": "dcd3eedd2418b01f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T13:02:00.424418Z",
     "start_time": "2024-12-25T13:02:00.417734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "\n",
    "def loaddata():\n",
    "    embedding_dim = 384\n",
    "    # 读文件的内容到这个列表中\n",
    "    text_lines = []\n",
    "    for file_path in glob(\"milvus_docs/en/faq/*.md\", recursive=True):\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_text = file.read()\n",
    "        text_lines += file_text.split(\"# \")\n",
    "        #表示按标题切割，md语法\n",
    "        \n",
    "    milvus_client = MilvusClient(uri='http://localhost:19530',db_name='learn')\n",
    "    collection_name = \"my_rag_collection\"\n",
    "    \n",
    "    if milvus_client.has_collection(collection_name):\n",
    "        milvus_client.drop_collection(collection_name)\n",
    "    \n",
    "    milvus_client.create_collection(collection_name=collection_name, dimension=embedding_dim,\n",
    "     metric_type=\"IP\",#指定向量相似度计算的度量类型为内积（Inner Product）。内积用于计算向量之间的相似度，适用于需要最大化相似度的场景\n",
    "     consistency_level=\"Strong\",#指定数据一致性级别为强一致性（Strong Consistency）。强一致性保证所有读操作都能读取到最新的写入数据。\n",
    "      )\n",
    "    data = []\n",
    "    for i, text in enumerate(text_lines):\n",
    "        data.append({\"id\":i, \"vector\": encode_document(text), \"text\": text})\n",
    "    milvus_client.insert(collection_name=collection_name, data=data)\n",
    "    "
   ],
   "id": "e4b9b2ca9ac23faf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T13:02:05.227346Z",
     "start_time": "2024-12-25T13:02:02.202616Z"
    }
   },
   "cell_type": "code",
   "source": "loaddata()",
   "id": "ccd1e384d5f521e5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "执行成功了，表示数据成功导入，去attu里面看一下可以看到数据已经在collection里了",
   "id": "f812ff6795cefeaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "下一步构建RAG\n",
    "这里先写一个简单的html界面，用于测试，然后用flask部署。"
   ],
   "id": "e978a39c2373a33c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实现Flask接口",
   "id": "bccb1f9a025b56e2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from flask import Flask, render_template_string, request\n",
    "from pymilvus import MilvusClient\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!doctype html>\n",
    "<html lang=\"en\" data-bs-theme=\"auto\">\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>简陋的测试</title>\n",
    "</head>\n",
    "\n",
    "<body class=\"bg-body-tertiary\">\n",
    "\n",
    "    <div class=\"container\">\n",
    "        <main>\n",
    "            <div class=\"py-5 text-center\">\n",
    "                <h2>RAG DEMO</h2>\n",
    "                <p class=\"lead\">救救软件课设</p>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"row mb-3\">\n",
    "                <h4 class=\"mb-3\">请输入您的问题</h4>\n",
    "                <form>\n",
    "                    <div class=\"row mb-3\">\n",
    "                        <div class=\"col-12\">\n",
    "                            <label for=\"question\" class=\"form-label\">问题内容</label>\n",
    "                            <input type=\"text\" class=\"form-control\" id=\"question\" required\n",
    "                                value=\"How is data stored in milvus?\">\n",
    "                        </div>\n",
    "                    </div>\n",
    "\n",
    "                    <button id=\"submit\" class=\"w-100 btn btn-primary btn-lg\" type=\"button\">提问</button>\n",
    "                </form>\n",
    "            </div>\n",
    "            <div class=\"row mb-3\">\n",
    "                <h4 class=\"mb-3\">Prompt</h4>\n",
    "                <textarea id=\"prompt\" rows=\"10\" style=\"margin-left: 10px;margin-right: 10px;\">请先提问</textarea>\n",
    "            </div>\n",
    "            <div class=\"row mb-3\">\n",
    "                <h4 class=\"mb-3\">回复</h4>\n",
    "                <div id=\"response\">请先提问</div>\n",
    "            </div>\n",
    "        </main>\n",
    "\n",
    "        <footer class=\"my-5 pt-5 text-body-secondary text-center text-small\">\n",
    "            <p class=\"mb-1\">&copy;2024 易俊哲</p>\n",
    "        </footer>\n",
    "    </div>\n",
    "    <script>\n",
    "        $('#submit').click(function () {\n",
    "            $.post('/', { \"question\": $('#question').val() }, function (data) {\n",
    "                $('#prompt').val(data.prompt)\n",
    "                $('#response').html(data.ai_answer)\n",
    "            }, 'json')\n",
    "        });\n",
    "    </script>\n",
    "\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return render_template_string(html_template)\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def indexPost():\n",
    "    question = request.form.get('question')\n",
    "\n",
    "    milvus_client = MilvusClient(uri='http://localhost:19530', db_name='learn')\n",
    "    collection_name = \"my_rag_collection\"\n",
    "\n",
    "    emb = encode_query(question)  # Encode the user's question into a vector\n",
    "    # Search the database for relevant documents\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[emb],\n",
    "        limit=3,  # Return 3 reference results\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "\n",
    "    # Concatenate the returned documents into a single string as context\n",
    "    context = \"\\n\".join([res[\"entity\"][\"text\"] for res in search_res[0]])\n",
    "\n",
    "    # Use LangChain to call the LLM and generate a response\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "\n",
    "    USER_PROMPT = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the local LLM model using LangChain\n",
    "    ollama = Ollama(base_url='http://localhost:11434', model=\"qwen:7b\")\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", SYSTEM_PROMPT), (\"user\", USER_PROMPT)]\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | ollama | parser\n",
    "\n",
    "    return json.dumps({\n",
    "        \"prompt\": USER_PROMPT,\n",
    "        \"ai_answer\": chain.invoke({}),  # Invoke the chain to get the response from the LLM\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ],
   "id": "5c39d61c5ecbb29c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from flask import Flask,render_template,request\n",
    "from pymilvus import MilvusClient\n",
    "from glob import glob\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate #注意引用了这个包\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return render_template(\"htmltest.html\")\n",
    "\n",
    "@app.route('/',methods=['POST'])\n",
    "def indexPost():\n",
    "    question = request.form.get('question')\n",
    "\n",
    "    milvus_client = MilvusClient(uri='http://localhost:19530',db_name='learn')\n",
    "    collection_name = \"my_rag_collection\"\n",
    "\n",
    "    emb = encode_query(question) #这里把用户输入的问题转为一个向量值\n",
    "    # 通过数据库搜索与问题相关的资料\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[\n",
    "            emb\n",
    "        ], \n",
    "        limit=3,  # 返回3个参考结果\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}}, \n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "\n",
    "    # 将返回参考资料拼成一个大字符串，作为参考资料\n",
    "    context = \"\\n\".join([\n",
    "        res[\"entity\"][\"text\"]for res in search_res[0]\n",
    "    ])\n",
    "\n",
    "    # 接下来用langchain调用LLM生成回复\n",
    "    SYSTEM_PROMPT  = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "\n",
    "    USER_PROMPT = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    # 使用LangChain调用本地搭建的LLM模型，我的电脑上是qwen:7b\n",
    "    ollama = Ollama(base_url='http://localhost:11434', model=\"qwen:7b\")\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # prompt\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", SYSTEM_PROMPT), (\"user\", USER_PROMPT)]\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | ollama | parser\n",
    "\n",
    "    return json.dumps({\n",
    "        \"prompt\":USER_PROMPT,\n",
    "        \"ai_answer\":chain.invoke({}), # 这里invoke会将前面的SYSTEM_PROMPT和USER_PROMPT组装成prompt，提交给LLM，并解析LLM的响应\n",
    "    })\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ],
   "id": "11fb1e4ab0377541",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
